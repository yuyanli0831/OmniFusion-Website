<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@100;400;700&display=swap" rel="stylesheet">
    <title>OmniFusion</title>
    <style>
        * {
            font-family: 'Poppins', sans-serif;
        }

        .abstract {
            text-align: justify;
        }

        a {
            text-decoration: none;
        }

        h1 {
            font-weight: bold;
        }

        .wrapper {
            text-align: center;

        }
    </style>
</head>

<body>

    <nav class="sticky-top navbar navbar-expand-lg navbar-light bg-light">
        <div class="container">
            <a class="navbar-brand" href="/">OmniFusion</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
                aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav">
                    <li class="nav-item">
                        <strong><a class="nav-link" href="#">Paper</a></strong>
                    </li>
                    <li class="nav-item">
                        <strong><a class="nav-link" href="https://arxiv.org/abs/2203.00838">arXiv</a></strong>
                    </li>
                    <li class="nav-item">
                        <strong><a class="nav-link" href="https://github.com/yuyanli0831/OmniFusion">GitHub</a></strong>
                    </li>

                </ul>
            </div>

        </div>
    </nav>


    <div class="container">
        <div class="row">
            <div class="col-md-12 mt-5">
                <h1 class="text-center">OmniFusion</h1>
                <h2 class="text-center">360 Monocular Depth Estimation via Geometry-Aware Fusion</h2>
                <!-- <h4 class="text-center">
                    <a href="#">Yuyan Li</a><sup>1</sup>,
                    <a href="#">Zhixin Yan</a><sup>2</sup>,
                    <a href="#">Ye Duan</a><sup>1</sup>
                    <a href="#">Liu Ren</a><sup>2</sup>,
                </h4>
                <p class="text-center">
                    <sup>1</sup>University of Missouri, <sup>2</sup>Bosch Research
                </p>
                <p class="text-center">
                    <img width="200" class="p-2" src="images/mizzou.png" alt="">
                    <img width="200" class="p-2" src="images/bnosch.png" alt="">
                </p> -->
                <div class="wrapper">
                    <div class="btn-group btn-group-lg mr-2 text-center" role="group" aria-label="First group">
                        <!-- <a href="https://ieeexplore.ieee.org/document/9665896" class="btn btn-disable btn-outline-primary">Paper</a> -->
                        <a href="https://arxiv.org/abs/2202.01323" class="btn btn-outline-primary">arXiv</a>
                        <a href="https://github.com/yuyanli0831/OmniFusion" class="btn btn-outline-primary">GitHub</a>
                    </div>
                </div>
                <h2>Abstract</h2>
                <p class="abstract">
                    A well-known challenge in applying deep-learning methods to omnidirectional images is spherical
                    distortion. In dense regression tasks such as depth estimation, where structural details are
                    required, using a vanilla CNN layer on the distorted 360 image results in undesired information
                    loss. In this paper, we propose a 360 monocular depth estimation pipeline, OmniFusion, to tackle the
                    spherical distortion issue. Our pipeline transforms a 360 image into less-distorted perspective
                    patches (i.e. tangent images) to obtain patch-wise predictions via CNN, and then merge the
                    patch-wise results for final output. To handle the discrepancy between patch-wise predictions which
                    is a major issue affecting the merging quality, we propose a new framework with the following key
                    components. First, we propose a geometry-aware feature fusion mechanism that combines 3D geometric
                    features with 2D image features to compensate for the patch-wise discrepancy. Second, we employ the
                    self-attention-based transformer architecture to conduct a global aggregation of patch-wise
                    information, which further improves the consistency. Last, we introduce an iterative depth
                    refinement mechanism, to further refine the estimated depth based on the more accurate geometric
                    features. Experiments show that our method greatly mitigates the distortion issue, and achieves
                    state-of-the-art performances on several 360 monocular depth estimation benchmark datasets.</p>

                <!-- <p class="text-center">
                    <img style="max-width: 800px; width: 100%;" src="images/poster.png" alt="">
                </p> -->

                <figure style="max-width: 800px; width: 100%; display: block; margin: 0 auto;" class="figure mb-5 mt-5">
                    <img src="images/poster.jpg" class="figure-img img-fluid rounded"
                        alt="General diagram of OmniFusion.">
                    <figcaption class="figure-caption">
                        <strong>Fig. </strong>Our method, Omnifusion, produces high-quality dense depth from a monocular
                        ERP input. Our method uses a set of N perspective patches (i.e. tangent images) to represent the
                        ERP image, and fuse the image features with 3D geometric features to improve the estimation of
                        the merged depth map. The corresponding camera poses of the tangent images are shown in the
                        middle row.
                    </figcaption>
                </figure>

                <figure style="max-width: 800px; width: 100%; display: block; margin: 0 auto;" class="figure mb-5 mt-5">
                    <img src="images/full_pipeline.jpg" class="figure-img img-fluid rounded"
                        alt="General pipeline of OmniFusion.">
                    <figcaption class="figure-caption">
                        <strong>Fig. </strong>A general pipeline of OmniFusion. We propose (1) an effective geometry aware 
                        fusion module to mitigate patch wise discrepancy; (2) A transformer integrated to leverage global context;
                        (3) An iterative refining scheme to recover structural detail.
                    </figcaption>
                </figure>

                <figure style="max-width: 800px; width: 100%; display: block; margin: 0 auto;" class="figure mb-5 mt-5">
                    <img src="images/results.jpg" class="figure-img img-fluid rounded"
                        alt="General diagram of OmniFusion.">
                    <figcaption class="figure-caption">
                        <strong>Fig. </strong>Qualitative results on Stanford2D3D, Matterport3D and 360D.
                    </figcaption>
                </figure>

                <figure style="max-width: 800px; width: 100%; display: block; margin: 0 auto;" class="figure mb-5 mt-5">
                    <img src="images/results-2.png" class="figure-img img-fluid rounded"
                        alt="General diagram of OmniFusion.">
                    <figcaption class="figure-caption">
                        <strong>Fig. </strong>Qualitative comparisons regarding individual components. The top row shows
                        the visual comparisons in depth maps, and the bottom row shows the visual comparisons of the
                        corresponding error maps between the predicted depth maps. The middle two rows show the close-up
                        views of the highlighted areas in the top and bottom rows, respectively.
                    </figcaption>
                </figure>

            </div>
        </div>

        <div class="row">
            <h2>Cite</h2>
            <div class="bg-light p-4">
                <pre>@inproceedings{li2022omnifusion,
        title={Omnifusion: 360 monocular depth estimation via geometry-aware fusion},
        author={Li, Yuyan and Guo, Yuliang and Yan, Zhixin and Huang, Xinyu and Duan, Ye and Ren, Liu},
        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
        pages={2801--2810},
        year={2022}
        }
</pre>
            </div>

        </div>
    </div>

    <div class="container">
        <!-- <p class="text-center p-5">
            Copyright Â© Yuyan Li 2022
        </p> -->
    </div>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"
        integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.min.js"
        integrity="sha384-QJHtvGhmr9XOIpI6YVutG+2QOK9T+ZnN4kzFN1RtK3zEFEIsxhlmWl5/YESvpZ13"
        crossorigin="anonymous"></script>

</body>

</html>